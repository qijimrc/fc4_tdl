## <center> 基于强化学习时序差分的对弈算法</center>
### <center>---- 模拟玩家风格以预测棋局结果</center>


### 1. 介绍
  本文实现了一个基于强化学习时序差分算法的双Agent自我对弈系统[1-3]。在给定四子棋环境下，两个Agent基于该算法进行自我对弈，每一轮对弈最大化自己获胜的期望目标，经过多轮次的对弈得以学习各自的最优对弈策略最终实现收敛。

  基于上述系统，本文还实现了基于给定的数据集的玩家风格模拟。 给定数据集：每个样本包括若干对弈步骤后的棋局状态和双方最终对弈结果，我们训练系统Agents模拟双方对弈风格，实现给定棋局状态下的最终对弈结果预测。

-----------------------------------------------------------------------------------------------

### 2. 方法

以下部分，我们首先给出四子棋问题的强化学习定义；其次详细介绍基于强化学习时序差分算法的自我对弈问题建模，然后对给定数据集下玩家风格模拟方法进行描述。

#### 2.1 基于n元组的四子棋问题定义

##### 2.1.1 四子棋
四子棋是一种包含6行7列的棋盘对弈游戏。从初始的空棋盘状态开始，玩家双方依次执黄色棋子和红色棋子在棋盘上空白的位置落子，首先在在横、纵或对角线方向上有几方四个子相连的玩家获胜。其中一条重要规则是：每个玩家落子时仅能在某一列的最下方空白位置落子（重力原则）。

四子棋的每个棋盘位置均有4种状态，所以棋局总共的状态多达$4.5\times 10^{12}$个，搜索空间极大。

##### 2.1.2 问题定义

###### n元组
我们使用n元组来定义每一个四子棋棋盘的状态。

一个n元组是一个长度为n的样本点序列$T=(\tau_1, \tau_2, ..., \tau_{n-1})$，其表示从左下第一个棋盘位置开始，所有棋盘位置的序号序列，即$\tau_i\in \{0,1,...,41\}$。

###### 状态
根据当前棋盘位置的落子情况，每一个位置的状态$s_t[\tau_i]\in{0,...,P-1}$定义为：

<center>$s_t[\tau_i]=\begin{cases} 0: empty and not reachable\\ 1:Yellow\\ 2: Red\\ 3: empty and reachable \end{cases}$</center>

其中，Yellow和Red分别表示玩家一和玩家二，reachable表示当前位置是否合法。

基于上述n元组的定义，我们将每一个棋局的状态定义为n元组各个棋盘位置状态的4进制加和：

<center>$i=\sum_{j=0}^{n-1}s_t[\tau_j]P^j$</center>

$i$表示第$i$个棋局状态的序号。

###### 镜像棋局状态
由于四子棋等棋盘是基于中间列左右对称的，故当出现左、右两侧的棋局和右、左两侧的棋局状态一致时，可视为同一状态。所以我们不需要从0到41计算42个棋子的总状态，而仅需以中轴列开始，分别从(0,3)到(5,0)向左上方向；和从(0,3)到(5,6)右上方向计算两次状态值，将两个值视为同一状态的表示即可。

###### 价值函数
对于每一个棋局状态，我们定义一个$d$维的权重向量$\vec{w_i}\in\mathbb{R}^d$，并定义该状态下的价值函数为：

<center>$V(s_t)=\sigma(\sum_{j=0}^{d}w_{ij})$</center>

其中$\sigma$是激活函数，我们设置$\sigma=tanh$，权重向量$\vec{w_i}$是可学习参数。

###### 奖励
我们设置通常意义上的对弈奖励：对于每一轮对弈，若Agent1获胜则获得奖励+1, 另外一个Agent2获得奖励-1；反之Agent1获得奖励-1，Agent2获得奖励+1；当出现平局时，每个Agent获得奖励为0.

基于上述问题定义，我们的目标是对弈双方的Agent学习最优策略，即每一个时刻的价值函数值，用以估计当前状态下下一步骤的行动策略。


#### 2.2 基于强化学习时序差分(Temporal Difference Learning)算法的自我对弈

##### 2.2.1 时序差分
给定价值函数的计算方法，时序差分学习(Temporal Difference Learning)方法基于当前状态下的价值函数估计$V(s_t)$，和下一状态下的价值函数估计$V(s_{t+1})$的残差，来学习整体参数的更新。定义下一状态得到的回报为$R(s_{t+1})$，则该残差定义为：

<center>$\delta_t=R(s_{t+1})+\gamma V(s_{t+1})-V(s_t)$</center>

其中$\gamma$表示折损因子，我们基于残差信号来更新当前时刻的权重向量：

<center>$w_{ij,t+1}=w_{ij,t}+\alpha\delta_t\partial_{w_{ij}}V(s_t)$</center>


##### 2.2.2 基于蒙特卡洛方法的资格追踪(Eligibility Tracing)

上述时序差分的方法存在一个缺陷：每一个episode中Agent仅能从最后一步获得回报影响，来更新参数，这将导致收敛速度异常缓慢。

一个解决办法是采用蒙特卡洛方法。采用蒙特卡洛方法时，Agent只有在完成每一个episode之后，每一步的价值函数参会更新，并且更新方式会依据当前episode得到的整体最终回报。

我们采用结合资格追踪的蒙特卡洛方法，具体为：对于每一个权重向量$\vec{w_i,t}$定义资格追踪向量$\vec{e_i,t}$，且

<center>$e_{ij,t}=\sum_{k=0}^t(\lambda\gamma)^{t-k}\partial_{w_{ij}}V(s_k)\\=\lambda\gamma e_{ij,t}+\partial_{w_{ij}}V(s_t)$</center>

<center>$e_{ij,0}=\partial_{w_{ij}}V(s_0)$</center>

其中$\lambda\in[0,1]$是追踪衰减因子，这里设置为1。

基于这个资格追踪方法，我们修改权重更新方式为：

<center>$w_{ij,t+1}=w_{ij,t}+\alpha\delta_t e_{ij,t}$</center>

#### 2.3. 模拟玩家风格预测棋局额结果

基于给定数据集：每个样本包括若干对弈步骤后的棋局状态和双方最终对弈结果，我们训练系统Agents模拟双方对弈风格，实现给定棋局状态下的最终对弈结果预测。

###### 初始化棋局状态
给定数据样本中的棋局棋局状态，我们直接将上述棋局环境初始化为该数据样本中的对弈状态，并按照规则(数子)选择Agent开始对弈。

###### 设置奖励
给定数据样本中的棋局最终胜负结果标签，我们重置奖励为：每一轮对弈结束时，若标签中获胜结果和实际对弈获胜结果不符时（e.g. 标签中Agent1获胜，而对弈结束时Agent1失败，Agent2获胜），给予每个Agent为-1的奖励；否则给予+1的奖励。

-----------------------------------------------------------------------------------------------


### 3. 实验
我们上述算法构建的系统能够在空白棋盘上进行Agents间的相互对弈。同时，我们在给定数据集上对数据上的Agent策略进行模拟实验，用以预测最终对弈结果。

同时，我们采用Python实现了可视化界面，用以展示对弈侧每一步行走及价值函数更新。

###### 实验设置
我们采用上述介绍的模型。我们将衰减因子$\gamma$设置为0.9；将探索概率设置为0.1，以表示当随机生成一个概率小于0.1时，则采用最优策略，反之采用探索策略随机试错游走；我们将学习率$\alpha$设置为0.004；将追踪衰减因子$\lambda$设置为1。每个权重向量的维度设置为$d=100$。

###### 实验数据
我们采用UCI机器学习数据集库中的[connect-4数据集](http://archive.ics.uci.edu/ml/datasets/connect-4)。该数据集包含 67557个实例，每个实例均为已落8个子的四子棋棋盘状态，对应的标签是 在玩家双方均采用最优策略的情况下，第一个玩家最终的输赢状态(win, loss, draw)。

我们对数据进行了超过5000000个episode的训练，得到收敛结果。

-----------------------------------------------------------------------------------------------

### 4. 模型解释分析
我们采用Python实现了Agents双方对弈的可视化界面，以展示对弈行走过程及价值函数更新。

#### 4.1 局部解释
局部解释是指 对模型在某个实例或某个实例所在的邻域内的决策行为的解释。

###### 游戏实例1
<center><img src="http://0.0.0.0:8000/next_move_agentRed.png" width = "400" height = "300" alt="棋局状态" align=center /></center>
<center>图1. 四子棋红方Agent在Current处落子后棋局状态</center>

图1所示，图中每个位置的数字表示在初始状态之后，每个Agent选择该位置落子的价值函数估计。该棋局状态指示第t个时间步骤红方Agent1将选择自身策略，将棋子落在current所示第4行第7列位置。

**分析**：在当前第t个时刻，Agent1(红方)有若干个落子策略可选择，其中，落子在第4行第7列位置的价值函数估计值最高，因为在这个状态下，Agent1只有落子在这个位置才能阻止对方（Agent2）获胜，且这种落子也更有利于后续决策：横向连成4子以取胜。



###### 游戏实例2
<center><img src="http://0.0.0.0:8000/next_move_agentRed1.png" width = "400" height = "300" alt="棋局状态" align=center /></center>
<center>图2. 四子棋红方Agent在Current处落子后棋局状态</center>

图2所示，图中每个位置的数字表示在初始状态之后，每个Agent选择该位置落子的价值函数估计。该棋局状态指示第t个时间步骤红方Agent1将选择自身策略，将棋子落在current所示第6行第3列位置。

**分析**：在当前第t个时刻，Agent1(红方)有若干个落子策略可选择，其中，落子在第6行第3列位置的价值函数估计值最高，在这个状态下，Agent1只有落子在这个位置才能阻止对方（Agent2）获胜。

#### 4.2 全局解释
全局解释是指在充分分析了模型之后，对模型行为进行归纳和总结，尝试生成可以指导人类棋手的策略规则，即尝试通过可解释的模型进 行知识发现。

**分析1**：在时间差分的强化学习算法中，两个Agent本身即在学习最优的决策策略，以获得最终的胜利，故该策略下对每一种棋局状态均有一个合适的价值估计，该估计是基于棋局结束时状态反馈产生的强化结果，能够很好的指示每个位置的行走策略。

**分析2**：使用时间强化学习的方式学习Agent对弈策略，需要不断的试错来逐步形成自身的价值函数估计。这些试错能够对环境未知的情况进行很好的建模，而不需要实现对环境做任何假设及额外搜索，是符合人类学习行为的建模方式。




### 5. Reference
[1] Thill, Markus, et al. "Temporal difference learning with eligibility traces for the game connect four." 2014 IEEE Conference on Computational Intelligence and Games. IEEE, 2014.

[2] M. Thill, P. Koch, and W. Konen, “Reinforcement learning with n-tuples on the game Connect-4,” in Proc. Parallel Problem Solving From Nature (PPSN’2012), C. Coello Coello, V. Cutello et al., Eds. Heidelberg: Springer, 2012, pp. 184–194.

[3] S. Bagheri, M. Thill, P. Koch, and W. Konen, “Online adaptable learning rates for the game Connect-4,” 2014, submitted to: IEEE Trans. on Computational Intelligence and AI in Games.

